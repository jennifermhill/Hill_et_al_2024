{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612e033a",
   "metadata": {},
   "source": [
    "# Cytosim reporting for multiple parameter runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd292f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## By Jennifer Hill, 2021\n",
    "## Adapted from Akamatsu et al., 2020, eLife\n",
    "## Report solid, fiber, crosslinker, and arp2/3 positions, numbers, and states from cytosim runs\n",
    "\n",
    "#import packages and libraries\n",
    "import numpy as np\n",
    "from scipy.stats import kde\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from subprocess import Popen\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt  # plotting\n",
    "plt.style.use('seaborn-v0_8-colorblind') # set plot style\n",
    "plt.cool()                          # heatmap color scheme\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd5e2680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set variables to define what will be reported\n",
    "timestep = 0.0001\n",
    "report_solid = 'yes'\n",
    "report_fibers = 'no'\n",
    "report_xlinks = 'no'\n",
    "report_arp = 'no'\n",
    "\n",
    "save_pickles ='yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8512f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set output directory where results will be placed (same directory that contains run directories) and working directory\n",
    "output_dir = '/Users/jenniferhill/Documents/tubeZsavio/experiments/longbudtime_1400pN_output_12031735/'\n",
    "working_dir = '/Users/jenniferhill/Documents/tubeZsavio/'\n",
    "\n",
    "\n",
    "#set location of report executable\n",
    "report_loc = '/Users/jenniferhill/Documents/tubeZlemmy/bin/report'\n",
    "\n",
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72660744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prefix for figure filenames\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime('%Y%m%d')\n",
    "pref = date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e56975f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframes and figures folder, in the output folder specific to this set of simulations. will take a lot of space for lots of sets of simulations.\n",
    "    \n",
    "os.chdir(output_dir)\n",
    "\n",
    "if os.path.isdir('figures') == False:\n",
    "    os.mkdir('figures')\n",
    "\n",
    "if os.path.isdir('dataframes') == False:\n",
    "    os.mkdir('dataframes')\n",
    "    \n",
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "025905fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dictionaries for datasets and a list for run directories\n",
    "solid_allruns = dict() #bud positions\n",
    "\n",
    "fiber_forces_allruns = dict() #forces on actin filaments\n",
    "fiber_ends_allruns = dict() #positions of plus/minus ends\n",
    "\n",
    "xlinks_allruns = dict() #positions of xlinks and arp2/3s\n",
    "xlinks_state_allruns = dict() #binding state of xlinks\n",
    "xlinks_forces_allruns = dict() #forces on xlinks\n",
    "\n",
    "arp_allruns = dict() #positions of xlinks and arp2/3s\n",
    "arp_branches_allruns = dict() #branching angle of arp2/3s\n",
    "\n",
    "properties_allruns = dict()\n",
    "rundirs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee9302c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for rundir in os.listdir(output_dir):\n",
    "# for rundir in ['xlinks_output_7150999']:\n",
    "     if rundir.startswith('run0'):\n",
    "        os.chdir(output_dir+'/'+rundir)\n",
    "        #change directory to ['xlinks_output_7150999/run****_****']\n",
    "        if report_solid == 'yes':\n",
    "            subprocess.call([report_loc, 'solid', 'output=solid.txt']) #runs the command report_loc (bin/report) with args 'solid' and 'output=solid_jl.txt'\n",
    "            \n",
    "            solid = open('solid.txt', 'r')\n",
    "            solid_allruns[rundir] = solid.readlines() #read the lines of solid.txt into a list called run****_**** and add to the solid_allruns dictionary\n",
    "            solid.close()  \n",
    "        \n",
    "        if report_fibers == 'yes':\n",
    "            subprocess.call([report_loc, 'fiber:forces', 'output=fiber_forces.txt'])\n",
    "            subprocess.call([report_loc, 'fiber:ends', 'output=fiber_ends.txt'])\n",
    "            \n",
    "            fiber_forces = open('fiber_forces.txt', 'r')\n",
    "            fiber_forces_allruns[rundir] = fiber_forces.readlines() #read the lines of fiber_forces.txt into a list called run****_**** and add to the fiber_forces_allruns dictionary\n",
    "            fiber_forces.close()\n",
    "            \n",
    "            fiber_ends = open('fiber_ends.txt', 'r')\n",
    "            fiber_ends_allruns[rundir] = fiber_ends.readlines() #read the lines of fiber_ends.txt into a list called run****_**** and add to the fiber_ends_allruns dictionary\n",
    "            fiber_ends.close()   \n",
    "            \n",
    "        if report_xlinks == 'yes':\n",
    "            subprocess.call([report_loc, 'couple:state', 'output=couple.txt'])\n",
    "            subprocess.call([report_loc, 'couple', 'output=couple_state.txt']) \n",
    "            subprocess.call([report_loc, 'couple:bridge', 'output=couple_forces.txt'])\n",
    "\n",
    "            xlinks = open('couple.txt', 'r')\n",
    "            xlinks_allruns[rundir] = xlinks.readlines()\n",
    "            xlinks.close()\n",
    "            \n",
    "            xlinks_state = open('couple_state.txt', 'r')\n",
    "            xlinks_state_allruns[rundir] = xlinks_state.readlines()\n",
    "            xlinks_state.close()\n",
    "        \n",
    "            xlinks_forces = open('couple_forces.txt', 'r')\n",
    "            xlinks_forces_allruns[rundir] = xlinks_forces.readlines()\n",
    "            xlinks_forces.close()\n",
    "        \n",
    "        if report_arp == 'yes':\n",
    "            subprocess.call([report_loc, 'couple:state', 'output=couple_state.txt']) \n",
    "            subprocess.call([report_loc, 'couple:angle:arp23', 'output=couple_arp_angles.txt'])\n",
    "    \n",
    "            arp = open('couple_state.txt', 'r')\n",
    "            arp_allruns[rundir] = arp.readlines()\n",
    "            arp.close()\n",
    "            \n",
    "            arp_branch = open('couple_arp_angles.txt', 'r')\n",
    "            arp_branches_allruns[rundir] = arp_branch.readlines()\n",
    "            arp_branch.close()\n",
    "        \n",
    "        properties = open('properties.cmo', 'r')\n",
    "        properties_allruns[rundir] = properties.readlines() #read the lines of properties.cmo into a list called run****_**** and add to the properties_allruns dictionary\n",
    "        properties.close()\n",
    "        rundirs.append(rundir) #add the current rundir run****_**** to the list of run directories\n",
    "        print('finished reporting ' + rundir)\n",
    "        os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba110a0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "properties_dict_allruns = dict()\n",
    "\n",
    "for rundir in rundirs:\n",
    "    properties = properties_allruns[rundir] #make a list called properties that is the lines from properties.cmo for that run\n",
    "    properties_dict = dict()\n",
    "    for line in properties:\n",
    "        if '=' in line:\n",
    "            line = line.strip().split(' = ') \n",
    "            #get rid of spaces on either end of the line, convert line into a list with items separated by an = in the line\n",
    "            #ex: ' time       = 9;' strip -> 'time     = 9;' split(' = ') -> ['time    ','9;']\n",
    "            properties_dict[line[0].strip()] = line[-1].strip(';')\n",
    "            #for the item (parameter) in the properties dictionary, add whatever the value is for that run\n",
    "            #ex: line[0].strip() is 'time', the value for time in the properties dictionary is '9'\n",
    "        properties_dict_allruns[rundir] = properties_dict #append the properties dictionary for this run to the dictionary with properties for all runs\n",
    "    print('finished reading ' + rundir + ' properties')   \n",
    "        \n",
    "properties_df = pd.DataFrame.from_dict(properties_dict_allruns, orient = 'index')     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ce4645",
   "metadata": {},
   "source": [
    "## Parse bud positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ccd87ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m         solid_outputs_allruns\u001b[38;5;241m.\u001b[39mappend(all_outputs)\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinished parsing \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m rundir)\n\u001b[0;32m---> 33\u001b[0m     all_solid_outputs_allruns \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(solid_outputs_allruns, keys \u001b[38;5;241m=\u001b[39m rundirs,\n\u001b[1;32m     34\u001b[0m                                   names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     35\u001b[0m all_solid_outputs_allruns\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/concat.py:372\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    370\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    373\u001b[0m     objs,\n\u001b[1;32m    374\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m    375\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[1;32m    376\u001b[0m     join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[1;32m    377\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[1;32m    378\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[1;32m    379\u001b[0m     names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[1;32m    380\u001b[0m     verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[1;32m    381\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    382\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    383\u001b[0m )\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/concat.py:429\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    426\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "#parse bud position\n",
    "all_solid_outputs_allruns = pd.DataFrame()\n",
    "\n",
    "if report_solid == 'yes':\n",
    "    solid_outputs_allruns = []\n",
    "    for rundir in rundirs:\n",
    "        all_lines = solid_allruns[rundir] #all_lines is the list of lines read from solid.txt for this run\n",
    "        timepoints = []\n",
    "        outputs = []\n",
    "        for line in all_lines:\n",
    "            line = line.strip() #remove spaces on either end of each line in the list\n",
    "            if line.startswith('%'):\n",
    "                if line.startswith('% time'): #happens first for each timepoint\n",
    "                    time = float(line.split(' ')[-1]) \n",
    "                    timepoints.append(time) #for lines that start with % time, split the line into a list and append the time item (-1) as a float to the timepoints list\n",
    "                    solids = {} #resets solids to empty\n",
    "                elif line.startswith('% end'): #happens last for each timepoint\n",
    "                    df = pd.DataFrame.from_dict(solids, orient = 'index') #for lines that start with % end, make a dataframe from the values in solids (points x y and z)\n",
    "                    outputs.append(df) #append the dataframe of values to the outputs list\n",
    "            elif len(line.split()) > 0: #aka it contains actual data not empty space\n",
    "                [solid_class, solid_id, centroid_x, centroid_y, centroid_z,\n",
    "                point_x, point_y, point_z, idk1, idk2, idk3] = line.split() #split the line into a list of values and assign the values to the associated variables\n",
    "                solids[int(solid_id)] = {'xpos': float(point_x), 'ypos' : float(point_y),\n",
    "                                      'zpos' : float(point_z)}\n",
    "        #creates outputs: list of dataframes containing x, y, and z values for the solid at all time points (one dataframe for each time point)\n",
    "        all_outputs = pd.concat(outputs, keys = timepoints,\n",
    "                                names = ['time', 'id'])\n",
    "        #concatenates all dataframes in the list 'outputs' into a single dataframe with timepoints (one dataframe containing all timepoints)\n",
    "        solid_outputs_allruns.append(all_outputs)\n",
    "        \n",
    "        print( 'finished parsing ' + rundir)\n",
    "\n",
    "    all_solid_outputs_allruns = pd.concat(solid_outputs_allruns, keys = rundirs,\n",
    "                                  names = ['run', 'time', 'id'])\n",
    "all_solid_outputs_allruns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593cdf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recalibrate bud z position so it starts at z=0 and is given in nm\n",
    "if report_solid == 'yes':\n",
    "    all_solid_outputs_allruns['internalization'] = (all_solid_outputs_allruns['zpos'] + 0.14) * 1000\n",
    "    all_solid_outputs_allruns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8873ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if report_solid == 'yes':\n",
    "    if save_pickles=='yes':\n",
    "        all_solid_outputs_allruns.to_pickle(output_dir+'/dataframes/bud_positions.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16439e23",
   "metadata": {},
   "source": [
    "## Parse forces on all fibers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse the forces on all fibers\n",
    "all_fiber_forces = pd.DataFrame()\n",
    "\n",
    "if report_fibers == 'yes':\n",
    "    fiber_forces_outputs_allruns = [] #make a dictionary for runs where each run is a dataframe of fiber force outputs from that run\n",
    "    all_fiber_forces = []\n",
    "\n",
    "    for rundir in rundirs:\n",
    "        single_all_lines = fiber_forces_allruns[rundir] #define the item in the fiber_forces_allruns dictionary for run****_****\n",
    "        timepoints = []\n",
    "        outputs = []\n",
    "        for line in single_all_lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('%'):\n",
    "                if line.startswith('% start'): #add the timepoint to the timepoints list, re-empty singles\n",
    "                    time = float(line.split(' ')[-1])\n",
    "                    timepoints.append(time)\n",
    "                    singles = {}\n",
    "                elif line.startswith('% end'): #make a dataframe from the dictionary of force values, append it to the outputs list\n",
    "                    df = pd.DataFrame.from_dict(singles, orient = 'index')\n",
    "                    outputs.append(df)\n",
    "                    #print('finished parsing ' + rundir + ' timepoint ' + str(time))\n",
    "            elif len(line.split()) > 0: #for lines with data in them, split the data into individual values\n",
    "                [fiber_id, pt_index, xpos, ypos, zpos,\n",
    "                xforce, yforce, zforce, tension] = line.split()\n",
    "                singles[str(fiber_id)+'_'+str(pt_index)] = {'fiber_id' : int(fiber_id),'pt_index' : int(pt_index),\n",
    "                                                            'xpos': float(xpos), 'ypos' : float(ypos), 'zpos': float(zpos),\n",
    "                                                            'xforce' : float(xforce), 'yforce' : float(yforce),\n",
    "                                                            'zforce': float(zforce), 'tension': float(tension)}\n",
    "\n",
    "        #concatenate all outputs dataframes (values for a given timepoint) into a single dataframe with values for all timepoints\n",
    "        all_outputs = pd.concat(outputs, keys = timepoints,\n",
    "                                names = ['time', 'id'])\n",
    "        #magnitude of the force is sqrt of dir. vectors squared\n",
    "        all_outputs['force_magnitude'] = np.sqrt(np.square(all_outputs['xforce']) + \n",
    "                                                  np.square(all_outputs['yforce']) +\n",
    "                                                  np.square(all_outputs['zforce']))\n",
    "        #add the outputs dataframe for the run to the list of dataframes for all runs\n",
    "        fiber_forces_outputs_allruns.append(all_outputs)\n",
    "\n",
    "        print( 'finished parsing ' + rundir)\n",
    "\n",
    "    #concatenate dataframes from each run into a single dataframe with values for all timepoints for all runs\n",
    "    all_fiber_forces = pd.concat(fiber_forces_outputs_allruns, keys = rundirs,\n",
    "                                      names = ['run', 'time', 'id'])\n",
    "all_fiber_forces.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27339a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if report_fibers == 'yes':\n",
    "    if save_pickles=='yes':\n",
    "        all_fiber_forces.to_pickle(output_dir+'/dataframes/actin_positions_forces.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a86d1",
   "metadata": {},
   "source": [
    "## Parse fiber end positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bf1fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse fiber end positions\n",
    "all_fiber_ends = pd.DataFrame()\n",
    "\n",
    "if report_fibers == 'yes':\n",
    "    fiber_ends_outputs_allruns = []\n",
    "    for rundir in rundirs:\n",
    "        single_all_lines = fiber_ends_allruns[rundir] #define the item in the fiber_ends_allruns dictionary for run****_****\n",
    "        timepoints = []\n",
    "        outputs = []\n",
    "        for line in single_all_lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('%'): \n",
    "                if line.startswith('% time'): #add the timepoint to the timepoints list, re-empty singles\n",
    "                    time = float(line.split(' ')[-1])\n",
    "                    timepoints.append(time)\n",
    "                    singles = {}\n",
    "                elif line.startswith('% end'): #make a dataframe from the dictionary of endpoint values, append it to the outputs list\n",
    "                    df = pd.DataFrame.from_dict(singles, orient = 'index')\n",
    "                    outputs.append(df)\n",
    "                    # print 'finished parsing ' + rundir + ' timepoint ' + str(time)\n",
    "            elif len(line.split()) > 0: #for lines with data in them, split the data into individual values\n",
    "                [fiber_class, fiber_id, length, minus_state, minus_xpos, minus_ypos, minus_zpos,\n",
    "                minus_xdir, minus_ydir, minus_zdir, plus_state, plus_xpos, plus_ypos,\n",
    "                plus_zpos, plus_xdir, plus_ydir, plus_zdir] = line.split()\n",
    "                singles[int(fiber_id)] = {'fiber_id' : int(fiber_id), 'length':float(length),\n",
    "                                          'minus_state':int(minus_state), 'minus_xpos':float(minus_xpos),\n",
    "                                          'minus_ypos':float(minus_ypos), 'minus_zpos':float(minus_zpos),\n",
    "                                          'minus_xdir':float(minus_xdir), 'minus_ydir':float(minus_ydir),\n",
    "                                          'minus_zdir':float(minus_zdir), 'plus_state':int(plus_state),\n",
    "                                          'plus_xpos':float(plus_xpos), 'plus_ypos':float(plus_ypos),\n",
    "                                          'plus_zpos':float(plus_zpos), 'plus_xdir':float(plus_xdir),\n",
    "                                          'plus_ydir':float(plus_ydir), 'plus_zdir':float(plus_zdir)}\n",
    "\n",
    "        #concatenate all outputs dataframes (values for a given timepoint) into a single dataframe with values for all timepoints\n",
    "        all_outputs = pd.concat(outputs, keys = timepoints,\n",
    "                            names = ['time', 'id'])\n",
    "        #convert plus end position in cartesian coords to polar coords (just r)\n",
    "        all_outputs['plus_rpos'] = np.sqrt(np.square(all_outputs['plus_xpos']) +\n",
    "                                      np.square(all_outputs['plus_ypos']))\n",
    "\n",
    "        #convert zdir to degrees, oriented vertically such that +90 is POSITIVE orientation and -90 is negative orientation\n",
    "        all_outputs['zdir_deg_flip90'] = np.degrees((np.arccos(all_outputs['plus_zdir'])-(np.pi)/2))\n",
    "\n",
    "        #add the outputs dataframe for the run to the list of dataframes for all runs\n",
    "        fiber_ends_outputs_allruns.append(all_outputs)\n",
    "\n",
    "        print( 'finished parsing ' + rundir)\n",
    "\n",
    "    #concatenate dataframes from each run into a single dataframe with values for all timepoints for all runs\n",
    "    all_fiber_ends = pd.concat(fiber_ends_outputs_allruns, keys = rundirs,\n",
    "                                  names = ['run', 'time', 'id'])\n",
    "all_fiber_ends.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca8e1d",
   "metadata": {},
   "source": [
    "## Recalibrate fiber end positions\n",
    "### Center X, Y, Z, R positions around bud position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c5fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bud moves in x, y, and z during simulation, recalculate fiber x, y, and z positions to be relative to bud position\n",
    "all_fiber_ends_recal = pd.DataFrame()\n",
    "\n",
    "if report_fibers == 'yes' and report_solid == 'yes':\n",
    "    all_fiber_ends_recal = pd.merge(all_fiber_ends.reset_index(), all_solid_outputs_allruns.reset_index(), on = ['run','time'])\n",
    "\n",
    "    all_fiber_ends_recal['minus_xpos_recal'] = (all_fiber_ends_recal['minus_xpos'] - all_fiber_ends_recal['xpos'])*1000\n",
    "    all_fiber_ends_recal['minus_ypos_recal'] = (all_fiber_ends_recal['minus_ypos'] - all_fiber_ends_recal['ypos'])*1000\n",
    "    all_fiber_ends_recal['minus_zpos_recal'] = (all_fiber_ends_recal['minus_zpos'] + 0.03)*(-1000)\n",
    "\n",
    "    all_fiber_ends_recal['plus_xpos_recal'] = (all_fiber_ends_recal['plus_xpos'] - all_fiber_ends_recal['xpos'])*1000\n",
    "    all_fiber_ends_recal['plus_ypos_recal'] = (all_fiber_ends_recal['plus_ypos'] - all_fiber_ends_recal['ypos'])*1000\n",
    "    all_fiber_ends_recal['plus_zpos_recal'] = (all_fiber_ends_recal['plus_zpos'] +0.03)*(-1000)\n",
    "\n",
    "    all_fiber_ends_recal['plus_rpos_recal'] = np.sqrt(np.square(all_fiber_ends_recal['plus_xpos_recal']) +\n",
    "                                      np.square(all_fiber_ends_recal['plus_ypos_recal']))\n",
    "\n",
    "    all_fiber_ends_recal['minus_rpos_recal'] = np.sqrt(np.square(all_fiber_ends_recal['minus_xpos_recal']) +\n",
    "                                      np.square(all_fiber_ends_recal['minus_ypos_recal']))\n",
    "\n",
    "    all_fiber_ends_recal = all_fiber_ends_recal.drop(columns=['id_y', 'xpos', 'ypos', 'zpos', 'internalization'])\n",
    "    all_fiber_ends_recal = all_fiber_ends_recal.rename(index=str, columns={\"id_x\": \"id\"})\n",
    "    all_fiber_ends_recal = all_fiber_ends_recal.set_index(['run', 'time', 'id'])\n",
    "\n",
    "all_fiber_ends_recal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b8f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_pickles=='yes':\n",
    "    if report_fibers == 'yes' and report_solid == 'yes':\n",
    "        all_fiber_ends_recal.to_pickle(output_dir+'/dataframes/actin_plus_minus_ends_recal.pkl')\n",
    "    elif report_fibers == 'yes':\n",
    "        all_fiber_ends.to_pickle(output_dir+'/dataframes/actin_plus_minus_ends.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bud_positions, internalization: true z position in nm of tip of bud, starts at z=0 and increases\n",
    "#all_fiber_ends_recal, x,y,z,rpos_recal: true x,y,z,r position in nm of plus/minus ends of actin filaments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4200aa7b",
   "metadata": {},
   "source": [
    "## Report and parse crosslinkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse crosslinker positions\n",
    "all_couple_xlinks = pd.DataFrame()\n",
    "\n",
    "if report_xlinks == 'yes':\n",
    "    couple_xlinks_outputs_allruns = [] #make a dictionary for runs where each run is a dataframe of xlinks outputs from that run\n",
    "    for rundir in rundirs:\n",
    "        couple_all_lines = xlinks_allruns[rundir] #define couple_all_lines as the list of lines for run****_**** from xlinks_allruns dictionary\n",
    "        timepoints = []\n",
    "        outputs = []\n",
    "        for line in couple_all_lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('%'): #see same lines from parsing internalization\n",
    "                if line.startswith('% time'):\n",
    "                    time = float(line.split(' ')[-1])\n",
    "                    timepoints.append(time)\n",
    "                    couples_xlinks = {} #resets couples_xlinks to empty\n",
    "                elif line.startswith('% end'):\n",
    "                    df = pd.DataFrame.from_dict(couples_xlinks, orient = 'index') #creates a dataframe consisting of all the lines from the last time point\n",
    "                    outputs.append(df) #adds the dataframe to the outputs list\n",
    "                    # print 'finished parsing ' + rundir + ' timepoint ' + str(time)\n",
    "            elif line.startswith('2'): #line is for a couple of class 2, aka crosslinker\n",
    "                [couple_class, couple_id, bound_state, xpos, ypos, zpos, id_fiber1, abscissa1, id_fiber2, abscissa2] = line.split()\n",
    "                couples_xlinks[int(couple_id)] = {'bound_state' : int(bound_state), 'arp_id': int(couple_id), \n",
    "                                               'xpos': float(xpos), 'ypos': float(ypos), 'zpos': float(zpos), \n",
    "                                               'id_fiber1': int(id_fiber1), 'abscissa1': float(abscissa1), 'id_fiber2': int(id_fiber2), 'abscissa2': float(abscissa2)}\n",
    "                #for the list item in couples_xlinks defined by the couple_id, assign the values to the given names\n",
    "\n",
    "        #convert the list of outputs dataframes to a single dataframe with xlinks outputs for each timepoint\n",
    "        all_outputs = pd.concat(outputs, keys = timepoints,\n",
    "                            names = ['time', 'id'])\n",
    "\n",
    "        #add the dataframe with outputs for all time points for run****_**** to the list of outputs for each run\n",
    "        couple_xlinks_outputs_allruns.append(all_outputs)\n",
    "\n",
    "        print( 'finished parsing ' + rundir)\n",
    "\n",
    "    #convert the list of outputs for each run into a single dataframe containing all runs\n",
    "    all_couple_xlinks = pd.concat(couple_xlinks_outputs_allruns, keys = rundirs,\n",
    "                                  names = ['run', 'time', 'id'])\n",
    "\n",
    "all_couple_xlinks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a second dataframe for singly bound crosslinkers only\n",
    "singly_bound_xlinks = pd.DataFrame()\n",
    "\n",
    "if report_xlinks == 'yes':\n",
    "    singly_bound = []\n",
    "\n",
    "    #id_fiber is zero when xlink hand is not bound to a fiber, singly bound will have one id = 0 and the other id =/= 0\n",
    "    singly_bound.append(all_couple_xlinks.loc[np.logical_and(all_couple_xlinks['id_fiber1'] != 0, all_couple_xlinks['id_fiber2'] == 0)])\n",
    "    #append the dataframe made up of the rows from all_couple_xlinks where id_fiber1 =/= 0 and id_fiber2 = 0 to the list singly_bound\n",
    "    singly_bound.append(all_couple_xlinks.loc[np.logical_and(all_couple_xlinks['id_fiber1'] == 0, all_couple_xlinks['id_fiber2'] != 0)])\n",
    "    #append the dataframe made up of the rows from all_couple_xlinks where id_fiber1 = 0 and id_fiber2 =/= 0 to the list singly_bound\n",
    "\n",
    "    #concatenate the two dataframes in the list together and sort by the run index (sorts all other indices as well)\n",
    "    singly_bound_xlinks = pd.concat(singly_bound)\n",
    "    singly_bound_xlinks = singly_bound_xlinks.sort_index()\n",
    "singly_bound_xlinks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a third dataframe for active crosslinkers only\n",
    "active_xlinks = pd.DataFrame()\n",
    "\n",
    "if report_xlinks == 'yes':\n",
    "    active_xlinks = pd.DataFrame()\n",
    "    active_xlinks = all_couple_xlinks.loc[np.logical_and(all_couple_xlinks['id_fiber1'] != 0, all_couple_xlinks['id_fiber2'] != 0)]\n",
    "    #doubly bound will have both fiber_ids =/= 0\n",
    "active_xlinks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af4d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to pickles\n",
    "if save_pickles=='yes':\n",
    "    singly_bound_xlinks.to_pickle(output_dir+'/dataframes/singly_bound_xlinks.pkl')\n",
    "    active_xlinks.to_pickle(output_dir+'/dataframes/active_xlinks.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350fe481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse crosslinker states\n",
    "all_couple_xlinks_state = pd.DataFrame()\n",
    "\n",
    "if report_xlinks == 'yes':\n",
    "    xlinks_state_outputs_allruns = [] #make a dictionary for runs where each run is a dataframe of xlinks outputs from that run\n",
    "    for rundir in rundirs:\n",
    "        couple_all_lines = xlinks_state_allruns[rundir] #define couple_all_lines as the list of lines for run****_**** from xlinks_state_allruns dictionary\n",
    "        timepoints = []\n",
    "        outputs = []\n",
    "        for line in couple_all_lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('%'): #see same lines from parsing internalization\n",
    "                if line.startswith('% time'):\n",
    "                    time = float(line.split(' ')[-1])\n",
    "                    timepoints.append(time)\n",
    "                    couples_xlinks = {} #resets couples_xlinks dict to empty\n",
    "                elif line.startswith('% end'):\n",
    "                    df = pd.DataFrame.from_dict(couples_xlinks, orient = 'index') #creates a dataframe consisting of crosslinker line from the last time point\n",
    "                    outputs.append(df) #adds the dataframe to the outputs list\n",
    "                    # print 'finished parsing ' + rundir + ' timepoint ' + str(time)\n",
    "            elif line.startswith('crosslinker'): #line is for a crosslinkers not arp2/3\n",
    "                [couple, total, active, FF, AF, FA, AA] = line.split()\n",
    "                couples_xlinks[(0)] = {'couple' : str(couple), 'total': int(total), \n",
    "                                               'active': int(active), 'FF': int(FF), 'AF': int(AF), \n",
    "                                               'FA': int(FA), 'AA': int(AA)}\n",
    "                #for the dict item in couples_xlinks defined by the couple_id, assign the values to the given names\n",
    "\n",
    "        #convert the list of outputs dataframes to a single dataframe with xlinks outputs for each timepoint\n",
    "        all_outputs = pd.concat(outputs, keys = timepoints,\n",
    "                            names = ['time', 'id'])\n",
    "\n",
    "        #generate a bound column with the total number of crosslinkers bound in the network\n",
    "        all_outputs['bound'] = all_outputs['AF'] + all_outputs['FA'] + all_outputs['AA']\n",
    "\n",
    "        #add the dataframe with outputs for all time points for run****_**** to the list of outputs for each run\n",
    "        xlinks_state_outputs_allruns.append(all_outputs)\n",
    "\n",
    "        print( 'finished parsing ' + rundir)\n",
    "\n",
    "    #convert the list of outputs for each run into a single dataframe containing all runs\n",
    "    all_couple_xlinks_state = pd.concat(xlinks_state_outputs_allruns, keys = rundirs,\n",
    "                                  names = ['run', 'time', 'id'])\n",
    "\n",
    "all_couple_xlinks_state.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886d099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse crosslinker forces\n",
    "all_couple_xlinks_force = pd.DataFrame()\n",
    "\n",
    "if report_xlinks == 'yes':\n",
    "    xlinks_force_outputs_allruns = [] #make a dictionary for runs where each run is a dataframe of xlinks outputs from that run\n",
    "    for rundir in rundirs:\n",
    "        couple_all_lines = xlinks_forces_allruns[rundir] #define couple_all_lines as the list of lines for run****_**** from xlinks_forces_allruns dictionary\n",
    "        timepoints = []\n",
    "        outputs = []\n",
    "        for line in couple_all_lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('%'): #see same lines from parsing internalization\n",
    "                if line.startswith('% time'):\n",
    "                    time = float(line.split(' ')[-1])\n",
    "                    timepoints.append(time)\n",
    "                    couples_xlinks = {} #resets couples_xlinks dict to empty\n",
    "                elif line.startswith('% end'):\n",
    "                    df = pd.DataFrame.from_dict(couples_xlinks, orient = 'index') #creates a dataframe consisting of crosslinker line from the last time point\n",
    "                    outputs.append(df) #adds the dataframe to the outputs list\n",
    "                    # print 'finished parsing ' + rundir + ' timepoint ' + str(time)\n",
    "            elif line.startswith('2'): #line is for a couple of class crosslinker\n",
    "                [couple_class, couple_id, id_fiber1, abscissa1, id_fiber_2, abscissa2, force_nrm, \n",
    "                 xpos_hand1, ypos_hand1, zpos_hand1, xpos_hand2, ypos_hand2, zpos_hand2] = line.split()\n",
    "                couples_xlinks[int(couple_id)] = {'couple_class' : int(couple_class), 'couple_id': int(couple_id), \n",
    "                                               'id_fiber1': int(id_fiber1), 'abscissa1': float(abscissa1), 'id_fiber2': int(id_fiber2), 'abscissa2': float(abscissa2), \n",
    "                                               'force_nrm': float(force_nrm), 'xpos_hand1': float(xpos_hand1), 'ypos_hand1': float(ypos_hand1), 'zpos_hand1': float(zpos_hand1),\n",
    "                                                 'xpos_hand2': float(xpos_hand2), 'ypos_hand2': float(ypos_hand2), 'zpos_hand2': float(zpos_hand2),}\n",
    "                #for the dict item in couples_xlinks defined by the couple_id, assign the values to the given names\n",
    "\n",
    "        #convert the list of outputs dataframes to a single dataframe with xlinks outputs for each timepoint\n",
    "        all_outputs = pd.concat(outputs, keys = timepoints,\n",
    "                            names = ['time', 'id'])\n",
    "\n",
    "        #add the dataframe with outputs for all time points for run****_**** to the list of outputs for each run\n",
    "        xlinks_force_outputs_allruns.append(all_outputs)\n",
    "\n",
    "        print( 'finished parsing ' + rundir)\n",
    "\n",
    "    #convert the list of outputs for each run into a single dataframe containing all runs\n",
    "    all_couple_xlinks_force = pd.concat(xlinks_force_outputs_allruns, keys = rundirs,\n",
    "                                  names = ['run', 'time', 'id'])\n",
    "\n",
    "all_couple_xlinks_force.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge xlinks forces to active_xlinks\n",
    "if report_xlinks == 'yes':\n",
    "    force_nrm = all_couple_xlinks_force.loc[:,('force_nrm')]\n",
    "    active_xlinks[\"force_nrm\"] = force_nrm\n",
    "active_xlinks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab76751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to pickles\n",
    "if report_xlinks == 'yes':\n",
    "    if save_pickles=='yes':\n",
    "        all_couple_xlinks_state.to_pickle(output_dir+'/dataframes/xlinks_state.pkl')\n",
    "        all_couple_xlinks.to_pickle(output_dir+'/dataframes/all_xlinks.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca3187",
   "metadata": {},
   "source": [
    "## Report and parse Arp2/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse the positions of all and bound arp2/3s\n",
    "all_couple_arp = pd.DataFrame()\n",
    "\n",
    "if report_arp == 'yes':\n",
    "    couple_arp_outputs_allruns = [] #make a dictionary for runs where each run is a dataframe of arp2/3 outputs from that run\n",
    "    for rundir in rundirs:\n",
    "        couple_all_lines = arp_allruns[rundir] #define couple_all_runs as the list of lines for run****_**** from arp_allruns dictionary\n",
    "        timepoints = []\n",
    "        outputs = []\n",
    "        for line in couple_all_lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('%'): #see same lines from parsing internalization\n",
    "                if line.startswith('% time'):\n",
    "                    time = float(line.split(' ')[-1])\n",
    "                    timepoints.append(time)\n",
    "                    couples_arp = {} #resets couples_arp to empty\n",
    "                elif line.startswith('% end'):\n",
    "                    df = pd.DataFrame.from_dict(couples_arp, orient = 'index') #creates a dataframe consisting of all the lines from the last time point\n",
    "                    outputs.append(df) #adds the dataframe to the outputs list\n",
    "                    # print 'finished parsing ' + rundir + ' timepoint ' + str(time)\n",
    "            elif line.startswith('1'): #line is for a couple of class 1, aka arp2/3\n",
    "                [couple_class, couple_id, bound_state, xpos, ypos, zpos, id_fiber1, abscissa1, id_fiber2, abscissa2] = line.split()\n",
    "                couples_arp[int(couple_id)] = {'bound_state' : int(bound_state), 'arp_id': int(couple_id), \n",
    "                                               'xpos': float(xpos), 'ypos': float(ypos), 'zpos': float(zpos), \n",
    "                                               'id_fiber1': int(id_fiber1), 'abscissa1': float(abscissa1), 'id_fiber2': int(id_fiber2), 'abscissa2': float(abscissa2)}\n",
    "                #for the list item in couples_arp defined by the couple_id, assign the values to the given names\n",
    "\n",
    "        #convert the list of outputs dataframes to a single dataframe with arp2/3 outputs for each timepoint\n",
    "        all_outputs = pd.concat(outputs, keys = timepoints,\n",
    "                            names = ['time', 'id'])\n",
    "\n",
    "        #add the dataframe with outputs for all time points for run****_**** to the list of outputs for each run\n",
    "        couple_arp_outputs_allruns.append(all_outputs)\n",
    "\n",
    "        print( 'finished parsing ' + rundir)\n",
    "\n",
    "    #convert the list of outputs for each run into a single dataframe containing all runs\n",
    "    all_couple_arp = pd.concat(couple_arp_outputs_allruns, keys = rundirs,\n",
    "                                  names = ['run', 'time', 'id'])\n",
    "\n",
    "    #create a second dataframe for bound arp2/3 only\n",
    "    bound_arp = all_couple_arp.loc[all_couple_arp['id_fiber1'] != 0] #id_fiber1 is zero when arp2/3 is not bound to a fiber\n",
    "all_couple_arp.head() #show the first few lines fo the all_couple_arp dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a611c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse arp2/3 branch angles (angle in cos theta)\n",
    "all_arp_combined = pd.DataFrame()\n",
    "\n",
    "if report_arp == 'yes':\n",
    "    couple_arp_branches_outputs_allruns = []\n",
    "\n",
    "    for rundir in rundirs:\n",
    "        couple_branches_all_lines = arp_branches_allruns[rundir] \n",
    "        #couple_branches_all_lines is the item in the arp_branches_allruns dictionary for run****_****\n",
    "        timepoints = []\n",
    "        outputs = []\n",
    "\n",
    "        for line in couple_branches_all_lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('%'):\n",
    "                if line.startswith('% time'): #add the timepoint to the timepoints list, re-empty couples_arp_branch\n",
    "                    time = float(line.split(' ')[-1])\n",
    "                    timepoints.append(time)\n",
    "                    couples_arp_branch = {}\n",
    "                elif line.startswith('% end'): #make a dataframe from the dictionary of scal_prod values, append it to the outputs list\n",
    "                    df = pd.DataFrame.from_dict(couples_arp_branch, orient = 'index')\n",
    "                    outputs.append(df)\n",
    "            elif len(line.split()) > 0: #for lines with data in them, split the data into class, id, and scal_prod\n",
    "                [couple_class, couple_id, scal_prod] = line.split()\n",
    "                couples_arp_branch[int(couple_id)] = {'scal_prod' : float(scal_prod)}\n",
    "                #for the item in the dictionary called couple_id, add the scal_prod value\n",
    "\n",
    "        #concatenate all outputs dataframes (scal_prod values for a given timepoint) into a single dataframe with scal_prod values for all timepoints\n",
    "        all_outputs = pd.concat(outputs, keys = timepoints,\n",
    "                            names = ['time', 'id'])\n",
    "\n",
    "        #convert scal_prod values into a branch angle in degrees (scal_prod is dot product using unit vectors, so angle is cos-1(scal_prod)\n",
    "        all_outputs['branch_angle_deg'] = np.degrees(np.arccos(all_outputs['scal_prod']))\n",
    "        #add the outputs dataframe for the run to the list of dataframes for all runs\n",
    "        couple_arp_branches_outputs_allruns.append(all_outputs)\n",
    "        print( 'finished parsing ' + rundir)\n",
    "\n",
    "    #concatenate dataframes from each run into a single dataframe with values for all timepoints for all runs\n",
    "    all_couple_arp_branches = pd.concat(couple_arp_branches_outputs_allruns, keys = rundirs,\n",
    "                                  names = ['run', 'time', 'id'])\n",
    "\n",
    "    #merge the branching angle dataframe with the arp23 dataframe\n",
    "    #how=outer means that for the arps without information about branching, these arps are kept in the dataframe\n",
    "    all_arp_combined = pd.merge(all_couple_arp, all_couple_arp_branches, on = ['run','time', 'id'], how = 'outer')\n",
    "\n",
    "all_arp_combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd48343",
   "metadata": {},
   "outputs": [],
   "source": [
    "if report_arp == 'yes':\n",
    "    if save_pickles=='yes':\n",
    "        all_arp_combined.to_pickle(output_dir+'/dataframes/arp_positions_angles.pkl')\n",
    "        bound_arp.to_pickle(output_dir+'/dataframes/bound_arp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd2e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All done!')\n",
    "print('Plot using CME_plotting_single_rungroup.ipynb, CME_plotting_multi_rungroup.ipynb, or CME_summary_plotting.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d39ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
